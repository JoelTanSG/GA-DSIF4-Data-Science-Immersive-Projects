{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 1: Standardized Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 1\n",
    "\n",
    "Part 1 requires knowledge of basic Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and more schools in the USA are changing their math curriculum, lowering the standards required to make it easier for those struggling with math. Is the trend of students taking the SAT test showing a worsening performance towards math and a greater lean towards reading and writing? This short analysis would take a look at the math score, across the states and see if we can find any noticeable trends or patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-Data)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAT and ACT are standardized tests that many colleges and universities in the United States require for their admissions process. This score is used along with other materials such as grade point average (GPA) and essay responses to determine whether or not a potential student will be accepted to the university.\n",
    "\n",
    "The SAT has two sections of the test: Evidence-Based Reading and Writing and Math ([*source*](https://www.princetonreview.com/college/sat-sections)). The ACT has 4 sections: English, Mathematics, Reading, and Science, with an additional optional writing section ([*source*](https://www.act.org/content/act/en/products-and-services/the-act/scores/understanding-your-scores.html)). They have different score ranges, which you can read more about on their websites or additional outside sources (a quick Google search will help you understand the scores for each test):\n",
    "* [SAT](https://collegereadiness.collegeboard.org/sat)\n",
    "* [ACT](https://www.act.org/content/act/en.html)\n",
    "\n",
    "Standardized tests have long been a controversial topic for students, administrators, and legislators. Since the 1940's, an increasing number of colleges have been using scores from sudents' performances on tests like the SAT and the ACT as a measure for college readiness and aptitude ([*source*](https://www.minotdailynews.com/news/local-news/2017/04/a-brief-history-of-the-sat-and-act/)). Supporters of these tests argue that these scores can be used as an objective measure to determine college admittance. Opponents of these tests claim that these tests are not accurate measures of students potential or ability and serve as an inequitable barrier to entry. Lately, more and more schools are opting to drop the SAT/ACT requirement for their Fall 2021 applications ([*read more about this here*](https://www.cnn.com/2020/04/14/us/coronavirus-colleges-sat-act-test-trnd/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Background Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Feb 2021 California drafted a new set of guidelines for math in their school district. It hinted at bridging the preceived gap in social justice causes. [Testing results](https://www.nytimes.com/2021/11/04/us/california-math-curriculum-guidelines.html#:~:text=New%20York%20Times-,Testing%20results,-regularly%20show%20that) has constantly shown the USA to be lagging behind other developed countries when it comes to the subject.[source](https://www.nytimes.com/2021/11/04/us/california-math-curriculum-guidelines.html) \n",
    "\n",
    "In the commentary [\"After 30 years of reforms to improve math instruction, reason for hoped and dismay\"](https://www.brookings.edu/blog/brown-center-chalkboard/2021/02/04/after-30-years-of-reforms-to-improve-math-instruction-reasons-for-hope-and-dismay/) Heather C. Hill a Professor from Harvard University suggested that math teachers has improved over the years and there has been a modest improvement over the years. \n",
    "\n",
    "A top rated middle school in New York City, recently to much outcry and protest, removed its advanced math classes and instead put all students at the same classes. Other schools in Manhattan and other citites has taken similiar actions in hope of creating a more equitable education environment [source](https://nypost.com/2021/07/24/parents-want-honors-math-back-at-manhattan-schoolparents-demand-manhattan-middle-school-bring-back-honors-math-classes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Data\n",
    "\n",
    "There are 10 datasets included in the [`data`](./data/) folder for this project. You are required to pick **at least two** of these to complete your analysis. Feel free to use more than two if you would like, or add other relevant datasets you find online.\n",
    "\n",
    "* [`act_2017.csv`](./data/act_2017.csv): 2017 ACT Scores by State\n",
    "* [`act_2018.csv`](./data/act_2018.csv): 2018 ACT Scores by State\n",
    "* [`act_2019.csv`](./data/act_2019.csv): 2019 ACT Scores by State\n",
    "* [`act_2019_ca.csv`](./data/act_2019_ca.csv): 2019 ACT Scores in California by School\n",
    "* [`sat_2017.csv`](./data/sat_2017.csv): 2017 SAT Scores by State\n",
    "* [`sat_2018.csv`](./data/sat_2018.csv): 2018 SAT Scores by State\n",
    "* [`sat_2019.csv`](./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "* [`sat_2019_by_intended_college_major.csv`](./data/sat_2019_by_intended_college_major.csv): 2019 SAT Scores by Intended College Major\n",
    "* [`sat_2019_ca.csv`](./data/sat_2019_ca.csv): 2019 SAT Scores in California by School\n",
    "* [`sat_act_by_college.csv`](./data/sat_act_by_college.csv): Ranges of Accepted ACT & SAT Student Scores by Colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The datasets that would be in use \n",
    "would be the 2017 to 2019 SAT scores by states. The SAT datasets consists of 5 columns, the first which would be the index states the different states in the USA, the second, the participation rate, the third called EBRW (Evidence-Based Reading and Writing), the fourth Math and finally the total score which would be the sum of both EBRW and Math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outside Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your problem statement and your chosen datasets, spend some time doing outside research on state policies or additional information that might be relevant. Summarize your findings below. If you bring in any outside tables or charts, make sure you are explicit about having borrowed them. If you quote any text, make sure that it renders as being quoted. **Make sure that you cite your sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The average score for Math is 528, and the average score for EBRW is 523\"[source](https://blog.prepscholar.com/what-is-a-good-sat-score-a-bad-sat-score-an-excellent-sat-score)\n",
    "\n",
    "![image info](https://upload.wikimedia.org/wikipedia/commons/5/5d/SAT-math-by-race-ethnicity.png)\n",
    "This chart from wikipedia shows the SAT math results of the different races over time. Overall trends shows a modest but steady increment over the years.[source](https://en.wikipedia.org/wiki/SAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Challenges\n",
    "\n",
    "1. Manually calculate mean:\n",
    "\n",
    "    Write a function that takes in values and returns the mean of the values. Create a list of numbers that you test on your function to check to make sure your function works!\n",
    "    \n",
    "    *Note*: Do not use any mean methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average of a list of numbers\n",
    "def cal_mean(lst):\n",
    "    result = 0\n",
    "    for i in lst: #run a for loop to get the sum in the list\n",
    "        result+=i\n",
    "    return result/len(lst) # divide it by the length of the list to get the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_mean(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Manually calculate standard deviation:\n",
    "\n",
    "    The formula for standard deviation is below:\n",
    "\n",
    "    $$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$\n",
    "\n",
    "    Where $x_i$ represents each value in the dataset, $\\mu$ represents the mean of all values in the dataset and $n$ represents the number of values in the dataset.\n",
    "\n",
    "    Write a function that takes in values and returns the standard deviation of the values using the formula above. Hint: use the function you wrote above to calculate the mean! Use the list of numbers you created above to test on your function.\n",
    "    \n",
    "    *Note*: Do not use any standard deviation methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taking in a list of values and returning the standard deviation\n",
    "\n",
    "def standard_deviation(lst):\n",
    "    mu = cal_mean(lst) #Get the avg using the above function created\n",
    "    n = len(lst) #Declare a variable for the total count of values\n",
    "    \n",
    "    var = sum((x-mu)**2 for x in lst)/n   #list comprehension to get the variance\n",
    "    \n",
    "    return (var)**0.5 #Get the square root of the variance to return the Std Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.707825127659933"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_deviation(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data cleaning function:\n",
    "    \n",
    "    Write a function that takes in a string that is a number and a percent symbol (ex. '50%', '30.5%', etc.) and converts this to a float that is the decimal approximation of the percent. For example, inputting '50%' in your function should return 0.5, '30.5%' should return 0.305, etc. Make sure to test your function to make sure it works!\n",
    "\n",
    "You will use these functions later on in the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert String percentage to a float\n",
    "# 1st Remove the % from the string using .replace\n",
    "# 2nd convert it to a float\n",
    "# 3rd divide the float by a 100\n",
    "\n",
    "def percent_string_to_float(string):\n",
    "    return (float(string.replace('%',''))/100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2545"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_string_to_float('25.45%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 2\n",
    "\n",
    "Part 2 requires knowledge of Pandas, EDA, data cleaning, and data visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "#This would allow me to display dataframes side by side for ease of comparison later\n",
    "from IPython.display import display_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Cleaning\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "click to expand instructions\n",
    "</summary>\n",
    "Import the datasets that you selected for this project and go through the following steps at a minimum. You are welcome to do further cleaning as you feel necessary:\n",
    "1. Display the data: print the first 5 rows of each dataframe to your Jupyter notebook.\n",
    "2. Check for missing values.\n",
    "3. Check for any obvious issues with the observations (keep in mind the minimum & maximum possible values for each test/subtest).\n",
    "4. Fix any errors you identified in steps 2-3.\n",
    "5. Display the data types of each feature.\n",
    "6. Fix any incorrect data types found in step 5.\n",
    "    - Fix any individual values preventing other columns from being the appropriate type.\n",
    "    - If your dataset has a column of percents (ex. '50%', '30.5%', etc.), use the function you wrote in Part 1 (coding challenges, number 3) to convert this to floats! *Hint*: use `.map()` or `.apply()`.\n",
    "7. Rename Columns.\n",
    "    - Column names should be all lowercase.\n",
    "    - Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`).\n",
    "    - Column names should be unique and informative.\n",
    "8. Drop unnecessary rows (if needed).\n",
    "9. Merge dataframes that can be merged.\n",
    "10. Perform any additional cleaning that you feel is necessary.\n",
    "11. Save your cleaned and merged dataframes as csv files.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/joeltan/Desktop/GA/Project_1/assets/data/sat_2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1x/_l6x9fg16hxgmf1r3g0hhr3h0000gn/T/ipykernel_12787/4079375567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a DataFrame from each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_sat17\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_sat18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_sat19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/joeltan/Desktop/GA/Project_1/assets/data/sat_2017.csv'"
     ]
    }
   ],
   "source": [
    "# Declare a vairable for each csv filepath\n",
    "url_2017 = '/Users/joeltan/Desktop/GA/Project_1/assets/data/sat_2017.csv'\n",
    "url_2018 = '/Users/joeltan/Desktop/GA/Project_1/assets/data/sat_2018.csv'\n",
    "url_2019 = '/Users/joeltan/Desktop/GA/Project_1/assets/data/sat_2019.csv'\n",
    "\n",
    "# Create a DataFrame from each dataset\n",
    "df_sat17 = pd.read_csv(url_2017)\n",
    "df_sat18 = pd.read_csv(url_2018)\n",
    "df_sat19 = pd.read_csv(url_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sat scores by state 2017\n",
    "df_sat17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sat scores by state 2018\n",
    "df_sat18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sat scores by state 2019\n",
    "df_sat19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "print(f'sat_2017 missing values:\\n{df_sat17.isnull().sum()}\\n')\n",
    "print(f'sat_2018 missing values:\\n{df_sat18.isnull().sum()}\\n')\n",
    "print(f'sat_2019 missing values:\\n{df_sat19.isnull().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check overall values of tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017\n",
    "print('Sat 2017')\n",
    "df_sat17.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min value for math: 52 seems to be a mistake. As the lowest possible score is 400\n",
    "\n",
    "First step would be to find for all values below the lowest possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat17.loc[df_sat17['Math'] < 400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is only one row with an issue. To get the correct score, I'll take the difference of the total with the Evidence-Based Reading and Writing score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat17.loc[20,['Math']] = 1060-536\n",
    "\n",
    "#Check\n",
    "df_sat17.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Check\n",
    "df_sat17.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018\n",
    "print('Sat 2018')\n",
    "df_sat18.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2019\n",
    "print('Sat 2019')\n",
    "df_sat19.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row count for 2019 @ 53 is greater then 2017 or 2018 @ 51\n",
    "\n",
    "Explore further for unique rows (states) between each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the datasets for unique elements in the column State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df_17 and df_18 together by pd.concat\n",
    "df_17_18_diff = pd.concat([df_sat17,df_sat18])\n",
    "\n",
    "# remove all duplicates from the column 'State' to leave any unique rows\n",
    "df_17_18_diff.drop_duplicates(subset='State',keep=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17_18_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above returns a empty dataframe which means both 2017 and 2018 SAT dataset has the same declared States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for 2018 and 2019 datasets to check for the difference\n",
    "df_18_19_diff = pd.concat([df_sat18,df_sat19])\n",
    "df_18_19_diff.drop_duplicates(subset=['State'],keep=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18_19_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puerto Rico and Virgin Islands are the two States that are unique to 2019 dataset\n",
    "#### Dataset for 2019 SAT test results has two additional rows and since they've missing value for Participation Rate too\n",
    "#### I decided to drop them for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop both rows from the df_19 dataset.\n",
    "\n",
    "df_sat19.drop(df_sat19.index[df_sat19['State'] == 'Virgin Islands'],inplace=True)\n",
    "df_sat19.drop(df_sat19.index[df_sat19['State'] == 'Puerto Rico'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check\n",
    "df_sat19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the column types\n",
    "\n",
    "print(f'Sat 2017\\n{df_sat17.dtypes}\\n')\n",
    "print(f'Sat 2018\\n{df_sat18.dtypes}\\n')\n",
    "print(f'Sat 2019\\n{df_sat19.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert participation columns from object to floats\n",
    "# using .apply and lambda function\n",
    "\n",
    "df_sat17['Participation']= df_sat17['Participation'].apply(lambda x: percent_string_to_float(x))\n",
    "df_sat18['Participation']= df_sat18['Participation'].apply(lambda x: percent_string_to_float(x))\n",
    "df_sat19['Participation Rate']= df_sat19['Participation Rate'].apply(lambda x: percent_string_to_float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check column names for all 3 datasets\n",
    "\n",
    "print(f'2017: {df_sat17.columns}\\n')\n",
    "print(f'2018: {df_sat18.columns}\\n')\n",
    "print(f'2019: {df_sat19.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "df_sat17.rename(columns={'State':'state',\n",
    "                         'Participation':'participation_rate_2017',\n",
    "                         'Evidence-Based Reading and Writing':'ebrw_avg_score_2017',\n",
    "                         'Math':'math_avg_score_2017',\n",
    "                         'Total':'total_avg_score_2017'},inplace=True)\n",
    "\n",
    "df_sat18.rename(columns={'State':'state',\n",
    "                         'Participation':'participation_rate_2018',\n",
    "                         'Evidence-Based Reading and Writing':'ebrw_avg_score_2018',\n",
    "                         'Math':'math_avg_score_2018',\n",
    "                         'Total':'total_avg_score_2018'},inplace=True)\n",
    "\n",
    "df_sat19.rename(columns={'State':'state',\n",
    "                         'Participation Rate':'participation_rate_2019',\n",
    "                         'EBRW':'ebrw_avg_score_2019',\n",
    "                         'Math':'math_avg_score_2019',\n",
    "                         'Total':'total_avg_score_2019'},inplace=True)\n",
    "\n",
    "#Check\n",
    "print(f'2017: {df_sat17.columns}\\n')\n",
    "print(f'2018: {df_sat18.columns}\\n')\n",
    "print(f'2019: {df_sat19.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all 3 dataframe into one\n",
    "\n",
    "df_list = [df_sat17,df_sat18,df_sat19]\n",
    "\n",
    "df = reduce(lambda left,right: pd.merge(left,right,on='state'),df_list)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some checks on the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df.to_csv('./assets/data/merged_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>\n",
    "    Click to Expand Instructions\n",
    "</summary>\n",
    "\n",
    "Now that we've fixed our data, and given it appropriate names, let's create a [data dictionary](http://library.ucmerced.edu/node/10249). \n",
    "\n",
    "A data dictionary provides a quick overview of features/variables/columns, alongside data types and descriptions. The more descriptive you can be, the more useful this document is.\n",
    "\n",
    "Example of a Fictional Data Dictionary Entry: \n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**county_pop**|*integer*|2010 census|The population of the county (units in thousands, where 2.5 represents 2500 people).| \n",
    "|**per_poverty**|*float*|2010 census|The percent of the county over the age of 18 living below the 200% of official US poverty rate (units percent to two decimal places 98.10 means 98.1%)|\n",
    "\n",
    "[Here's a quick link to a short guide for formatting markdown in Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "Provided is the skeleton for formatting a markdown table, with columns headers that will help you create a data dictionary to quickly summarize your data, as well as some examples. **This would be a great thing to copy and paste into your custom README for this project.**\n",
    "\n",
    "*Note*: if you are unsure of what a feature is, check the source of the data! This can be found in the README.\n",
    "\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Dataset(year)|Description|\n",
    "|---|---|---|---|\n",
    "participation_rate_2017|float64|2017| The participation rate of students for the different states in 2017\n",
    "ebrw_avg_score_2017|int64|2017|Average score for Evidence-Based Reading and Writing (critical reading and writing makes up 2/3 of the score) \n",
    "math_avg_score_2017|int64|2017| Average score for the 2017 math section of the SAT test across the state\n",
    "total_avg_score_2017|int64|2017| The sum of both the Math score and the EBRW score for 2017. The minimum is 400 and the max 1,600.\n",
    "participation_rate_2018|float64|2018| The participation rate of students for the different states in 2018\n",
    "ebrw_avg_score_2018|int64|2018|Average score for Evidence-Based Reading and Writing (critical reading and writing makes up 2/3 of the score) \n",
    "math_avg_score_2018|int64|2018| Average score for the 2018 math section of the SAT test across the state\n",
    "total_avg_score_2018|int64|2018| The sum of both the Math score and the EBRW score for 2018. The minimum is 400 and the max 1,600.\n",
    "participation_rate_2019|float64|2019| The participation rate of students for the different states in 2019\n",
    "ebrw_avg_score_2019|int64|2019|Average score for Evidence-Based Reading and Writing (critical reading and writing makes up 2/3 of the score) \n",
    "math_avg_score_2019|int64|2019| Average score for the 2019 math section of the SAT test across the state\n",
    "total_avg_score_2019|int64|2019| The sum of both the Math score and the EBRW score for 2019. The minimum is 400 and the max 1,600.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand instructions</summary>\n",
    "    \n",
    "    Complete the following steps to explore your data. You are welcome to do more EDA than the steps outlined here as you feel necessary:\n",
    "1. Summary Statistics.\n",
    "2. Use a **dictionary comprehension** to apply the standard deviation function you create in part 1 to each numeric column in the dataframe.  **No loops**.\n",
    "    - Assign the output to variable `sd` as a dictionary where: \n",
    "        - Each column name is now a key \n",
    "        - That standard deviation of the column is the value \n",
    "        - *Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}`\n",
    "3. Investigate trends in the data.\n",
    "    - Using sorting and/or masking (along with the `.head()` method to avoid printing our entire dataframe), consider questions relevant to your problem statement. Some examples are provided below (but feel free to change these questions for your specific problem):\n",
    "        - Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Which states have the highest and lowest mean total/composite scores for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "        - Do any states show have >50% participation on *both* tests each year?\n",
    "        - Which colleges have the highest median SAT and ACT scores for admittance?\n",
    "        - Which California school districts have the highest and lowest mean test scores?\n",
    "    - **You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the different columns using the function created earlier and dictionary comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns.tolist()[1:] #remove the first column as it is an object type (State)\n",
    "\n",
    "sd = {c:standard_deviation(df[c]) for c in column_names}\n",
    "\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It is interesting to note that the manually calculated standard deviation is slightly different from the generated describe method. This is most likely to a rounding in the internal calculation from the .describe() method_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = df.describe()\n",
    "df_describe.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate trends on the data.\n",
    "\n",
    "- Which states have the highest and lowest participation rates for the 2017, 2019, and 2019 SAT? (using sort)\n",
    "\n",
    "    -[Highest Participation](#States-that-have-the-highest-participation-rates-from-2017-to-2019-SAT)\n",
    "    \n",
    "    -[Lowest Participation](#States-that-have-the-lowest-participation-rates-from-2017-to-2019-SAT)\n",
    "    \n",
    "    \n",
    "- Which states have the highest and lowest mean total/composite scores for the 2017, 2019, and 2019 SAT? (using sort)\n",
    "\n",
    "    -[Highest mean total composite scores](#States-that-have-the-highest-mean-total-composite-scores-from-2017-to-2019-SAT)\n",
    "    \n",
    "    -[Lowest mean total composite scores](#States-that-have-the-lowest-mean-total-composite-scores-from-2017-to-2019-SAT)\n",
    "\n",
    "\n",
    "- Which states has higher then the national average score for the years 2017, 2018, and 2019 SAT?(boolean filtering)\n",
    "\n",
    "    - [Math](#Which-states-has-higher-then-the-national-average-math-score-for-the-years-2017-2018-and-2019-SAT?)\n",
    "    \n",
    "    - [EBRW](#Which-states-has-higher-then-the-national-average-EBRW-score-for-the-years-2017-2018-and-2019-SAT?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display dataframes side by side\n",
    "def dsbs(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States that have the highest participation rates from 2017 to 2019 SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = df[['state','participation_rate_2017']].sort_values(by=['participation_rate_2017'],ascending=False).head(10)\n",
    "x2 = df[['state','participation_rate_2018']].sort_values(by=['participation_rate_2018'],ascending=False).head(10)\n",
    "x3 = df[['state','participation_rate_2019']].sort_values(by=['participation_rate_2019'],ascending=False).head(10)\n",
    "\n",
    "dsbs(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations on the top participation rate from 2017-2019\n",
    "1. Delaware, Connecticut and Michigan has 100% participation rate across the years\n",
    "2. Full participation rate has increased over time. From just 4 states in 2017 to 5 in 2018 and 8 in 2019\n",
    "3. Overall participation rates in the top 10 averaged higher over the three years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States that have the lowest participation rates from 2017 to 2019 SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df[['state','participation_rate_2017']].sort_values(by=['participation_rate_2017'],ascending=True).head(10)\n",
    "x2 = df[['state','participation_rate_2018']].sort_values(by=['participation_rate_2018'],ascending=True).head(10)\n",
    "x3 = df[['state','participation_rate_2019']].sort_values(by=['participation_rate_2019'],ascending=True).head(10)\n",
    "\n",
    "dsbs(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations on the bottom participation rate from 2017-2019\n",
    "1. There are 3 states that tie at the bottom with the lowest participation rate in 2017. They're: North Dakota, Mississippi, Iowa and Missouri\n",
    "2. North Dakota has been consistently the state with the lowest participation at only 20%\n",
    "3. While the participation rates of the top 10 states trend upwards, the bottom states has no significant changes remaining between 20% -40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States that have the highest mean total composite scores from 2017 to 2019 SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df[['state','total_avg_score_2017']].sort_values(by=['total_avg_score_2017'],ascending=False).head()\n",
    "x2 = df[['state','total_avg_score_2018']].sort_values(by=['total_avg_score_2018'],ascending=False).head()\n",
    "x3 = df[['state','total_avg_score_2019']].sort_values(by=['total_avg_score_2019'],ascending=False).head()\n",
    "\n",
    "dsbs(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique states from all 3 years y concatenating the dataframes and dropping duplicates\n",
    "x4 = pd.concat([x1,x2,x3])\n",
    "x4 = (x4['state'].drop_duplicates()).tolist()\n",
    "\n",
    "print('List of unique states across all three years:',x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations on the top scoring states from 2017-2019\n",
    "1. Minnesota and Wisconsin had the top 2 total composite score for all three years. Their results for 2019 is only different by a single point.\n",
    "2. The average total scores are consistently range bound between 1298 and 1260 inclusive\n",
    "3. States in the top 5 across the years are neighbouring states, geographically located at the mid-north of the country. [Original Map](http://alabamamaps.ua.edu/contemporarymaps/usa/basemaps/usstates1.jpg)\n",
    "![](./assets/images/us_states.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States that have the lowest mean total composite scores from 2017 to 2019 SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df[['state','total_avg_score_2017']].sort_values(by=['total_avg_score_2017'],ascending=True).head()\n",
    "x2 = df[['state','total_avg_score_2018']].sort_values(by=['total_avg_score_2018'],ascending=True).head()\n",
    "x3 = df[['state','total_avg_score_2019']].sort_values(by=['total_avg_score_2019'],ascending=True).head()\n",
    "        \n",
    "dsbs(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average total score of the bottom 5')\n",
    "print('2017',x1['total_avg_score_2017'].mean())\n",
    "print('2018',x2['total_avg_score_2018'].mean())\n",
    "print('2019',x3['total_avg_score_2019'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observation on the bottom 5 total average score across 2017-2019\n",
    "1. Delaware, Idaho and District of Columbia consistently rank at the bottom 5\n",
    "2. 2019 was the worst performing year for the bottom 5 states\n",
    "3. Geographically there is no observable patterns unlike the top 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which states has higher then the national average math score for the years 2017 2018 and 2019 SAT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average for each year\n",
    "avg_17_math = df['math_avg_score_2017'].mean()\n",
    "avg_18_math = df['math_avg_score_2018'].mean()\n",
    "avg_19_math = df['math_avg_score_2019'].mean()\n",
    "\n",
    "#Boolean filter merged dataframe\n",
    "avg_math_df_17 =df[df['math_avg_score_2017']>avg_17_math]\n",
    "avg_math_df_18 =df[df['math_avg_score_2018']>avg_18_math]\n",
    "avg_math_df_19 =df[df['math_avg_score_2019']>avg_19_math]\n",
    "\n",
    "#Filter only for the state columns and rename to the different years\n",
    "avg_math_df_17 =avg_math_df_17[['state']].rename(columns={'state':'2017'})\n",
    "avg_math_df_18 =avg_math_df_18[['state']].rename(columns={'state':'2018'})\n",
    "avg_math_df_19 =avg_math_df_19[['state']].rename(columns={'state':'2019'})\n",
    "\n",
    "frames = [avg_math_df_17,avg_math_df_18,avg_math_df_19]\n",
    "\n",
    "#combine them together and replace the NaN values for ease of comparison\n",
    "combine_math = pd.concat(frames).apply(lambda x: pd.Series(x.dropna().values))\n",
    "combine_math.fillna('',inplace=True)\n",
    "\n",
    "combine_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations\n",
    "The above show the states that are above the national math average score for 2017 to 2019\n",
    "\n",
    "1. 2017 had 25 states above the national average and this has since dropped to 18 in 2018 and 19 in 2019.\n",
    "\n",
    "\n",
    "2. It is interesting to note that states listed as the least educated states: Alabama, Arkansas, Louisiana, and Mississippi are above the national average. While states that are listed as the most educated are not in the any of the list: Maryland, Connecticut, with Vermont and Colorada making it above the national average only in 2017.[source](https://worldpopulationreview.com/state-rankings/least-educated-states)\n",
    "\n",
    "\n",
    "3. It is also noteworthy that states with higher participation rate do not appear in any of the above years. Does this takeaway weight from the theory that the environment plays a major factor in a person's ability to get a good education? Or are there outliers in the data that is pulling down the overall average score of these states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which states has higher then the national average EBRW score for the years 2017 2018 and 2019 SAT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average for each year\n",
    "avg_17_ebrw = df['ebrw_avg_score_2017'].mean()\n",
    "avg_18_ebrw = df['ebrw_avg_score_2018'].mean()\n",
    "avg_19_ebrw = df['ebrw_avg_score_2019'].mean()\n",
    "\n",
    "#Boolean filter merged dataframe\n",
    "avg_ebrw_df_17 =df[df['ebrw_avg_score_2017']>avg_17_ebrw]\n",
    "avg_ebrw_df_18 =df[df['ebrw_avg_score_2018']>avg_18_ebrw]\n",
    "avg_ebrw_df_19 =df[df['ebrw_avg_score_2019']>avg_19_ebrw]\n",
    "\n",
    "#Filter only for the state columns and rename to the different years\n",
    "avg_ebrw_df_17 =avg_ebrw_df_17[['state']].rename(columns={'state':'2017'})\n",
    "avg_ebrw_df_18 =avg_ebrw_df_18[['state']].rename(columns={'state':'2018'})\n",
    "avg_ebrw_df_19 =avg_ebrw_df_19[['state']].rename(columns={'state':'2019'})\n",
    "\n",
    "frames = [avg_ebrw_df_17,avg_ebrw_df_18,avg_ebrw_df_19]\n",
    "\n",
    "#combine them together and replace the NaN values for ease of comparison\n",
    "combine_ebrw = pd.concat(frames).apply(lambda x: pd.Series(x.dropna().values))\n",
    "combine_ebrw.fillna('',inplace=True)\n",
    "\n",
    "combine_ebrw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General observations\n",
    "1. It can be observed that states with above national average score for EBRW continue to do so across all three years with the exception of afew.\n",
    "\n",
    "\n",
    "2. Compared to the math national average results, the total number of states above the national average for EBRW is just one more then math.\n",
    "\n",
    "\n",
    "3. It is interesting to see that Colorado also made the list for 2017 but not so for subsequent years. Was there any natural or man-made disaster in Colorado after 2017 that is causing the students to fall behind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand instructions</summary>\n",
    "    \n",
    "There's not a magic bullet recommendation for the right number of plots to understand a given dataset, but visualizing your data is *always* a good idea. Not only does it allow you to quickly convey your findings (even if you have a non-technical audience), it will often reveal trends in your data that escaped you when you were looking only at numbers. It is important to not only create visualizations, but to **interpret your visualizations** as well.\n",
    "\n",
    "**Every plot should**:\n",
    "- Have a title\n",
    "- Have axis labels\n",
    "- Have appropriate tick labels\n",
    "- Text is legible in a plot\n",
    "- Plots demonstrate meaningful and valid relationships\n",
    "- Have an interpretation to aid understanding\n",
    "\n",
    "Here is an example of what your plots should look like following the above guidelines. Note that while the content of this example is unrelated, the principles of visualization hold:\n",
    "\n",
    "![](https://snag.gy/hCBR1U.jpg)\n",
    "*Interpretation: The above image shows that as we increase our spending on advertising, our sales numbers also tend to increase. There is a positive correlation between advertising spending and sales.*\n",
    "\n",
    "---\n",
    "\n",
    "Here are some prompts to get you started with visualizations. Feel free to add additional visualizations as you see fit:\n",
    "1. Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features.\n",
    "    - Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "    - Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative).\n",
    "2. Visualize distributions using histograms. If you have a lot, consider writing a custom function and use subplots.\n",
    "    - *OPTIONAL*: Summarize the underlying distributions of your features (in words & statistics)\n",
    "         - Be thorough in your verbal description of these distributions.\n",
    "         - Be sure to back up these summaries with statistics.\n",
    "         - We generally assume that data we sample from a population will be normally distributed. Do we observe this trend? Explain your answers for each distribution and how you think this will affect estimates made from these data.\n",
    "3. Plot and interpret boxplots. \n",
    "    - Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "    - Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "    - Each boxplot should:\n",
    "        - Only include variables of a similar scale\n",
    "        - Have clear labels for each variable\n",
    "        - Have appropriate titles and labels\n",
    "4. Plot and interpret scatter plots to view relationships between features. Feel free to write a custom function, and subplot if you'd like. Functions save both time and space.\n",
    "    - Your plots should have:\n",
    "        - Two clearly labeled axes\n",
    "        - A proper title\n",
    "        - Colors and symbols that are clear and unmistakable\n",
    "5. Additional plots of your choosing.\n",
    "    - Are there any additional trends or relationships you haven't explored? Was there something interesting you saw that you'd like to dive further into? It's likely that there are a few more plots you might want to generate to support your narrative and recommendations that you are building toward. **As always, make sure you're interpreting your plots as you go**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring correlation between Total Results, EBRW, and Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set xticks and yticks \n",
    "xtick= ['Participation','EBRW','Math','Total']\n",
    "ytick= ['Participation','EBRW','Math','Total']\n",
    "\n",
    "#set figure size\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "#heatmap for 2017 dataset\n",
    "heat_map_17 = plt.subplot(1,3,1)\n",
    "heat_map_17.set_title('SAT 2017 score')\n",
    "heat_map_17 = sns.heatmap(df_sat17.corr(),annot=True, vmin = -1,vmax = 1, cmap='Blues',cbar=False)\n",
    "plt.yticks(np.arange(4)+0.5,labels=ytick,rotation =0)\n",
    "plt.xticks(np.arange(4)+0.5,labels=xtick)\n",
    "\n",
    "#heatmap for 2018 dataset\n",
    "heat_map_18 = plt.subplot(1,3,2, sharey= heat_map_17)\n",
    "heat_map_18.set_title('SAT 2018 score')\n",
    "heat_map_18 = sns.heatmap(df_sat18.corr(),annot=True, vmin = -1,vmax = 1, cmap='Blues',cbar=False)\n",
    "plt.yticks(np.arange(4)+0.5,labels=ytick,rotation =-20)\n",
    "plt.xticks(np.arange(4)+0.5,labels=xtick)\n",
    "\n",
    "#heatmap for 2019 dataset\n",
    "heat_map_19 = plt.subplot(1,3,3, sharey= heat_map_18)\n",
    "heat_map_19.set_title('SAT 2019 score')\n",
    "heat_map_19 = sns.heatmap(df_sat19.corr(),annot=True, vmin = -1,vmax = 1, cmap='Blues',cbar=False)\n",
    "plt.yticks(np.arange(4),labels=ytick,rotation=-20)\n",
    "plt.xticks(np.arange(4)+0.5,labels=xtick)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "I wanted to see which score, math or EBRW was more correlated to the total which whould lean weight to students using one of the subjects to help with their overall total score. \n",
    "\n",
    "However it would seem no such relationship exist.\n",
    "\n",
    "_A side note, there is a negative correlation between Participation rate and the resulting scores in general. I notice this earlier while doing the exploratory data analysis, where states with high participation rates did not score well overall. This perhaps could be further explored in another project._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the range of Math scores vs EBRW scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tables to plot for the different years\n",
    "df_score17_isolated = df.loc[:,['state','math_avg_score_2017','ebrw_avg_score_2017']]\n",
    "df_score18_isolated = df.loc[:,['state','math_avg_score_2018','ebrw_avg_score_2018']]\n",
    "df_score19_isolated = df.loc[:,['state','math_avg_score_2019','ebrw_avg_score_2019']]\n",
    "\n",
    "#create figure\n",
    "fig = plt.figure()\n",
    "\n",
    "#set subplots\n",
    "ax0 = fig.add_subplot(1,3,1)\n",
    "ax1 = fig.add_subplot(1,3,2)\n",
    "ax2 = fig.add_subplot(1,3,3)\n",
    "\n",
    "#2017 boxplot\n",
    "df_score17_isolated.plot(kind='box',figsize=(17,5),ax=ax0,)\n",
    "ax0.set_title('2017 Math vs EBRW score')\n",
    "ax0.set_ylabel('score')\n",
    "ax0.set_xlabel('SAT sections')\n",
    "ax0.set_facecolor('#e1f7ed')\n",
    "\n",
    "#2018 boxplot\n",
    "df_score18_isolated.plot(kind='box',figsize=(17,5),ax=ax1)\n",
    "ax1.set_title('2018 Math vs EBRW score')\n",
    "ax1.set_ylabel('score')\n",
    "ax1.set_xlabel('SAT sections')\n",
    "ax1.set_facecolor('#e1f7ed')\n",
    "\n",
    "#2019 boxplot\n",
    "df_score19_isolated.plot(kind='box',figsize=(17,5),ax=ax2)\n",
    "ax2.set_title('2019 Math vs EBRW score')\n",
    "ax2.set_ylabel('score')\n",
    "ax2.set_xlabel('SAT sections')\n",
    "ax2.set_facecolor('#e1f7ed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "\n",
    "1. The range for the average math score across the states is consistently larger then the EBRW average score.\n",
    "2. The gap between the means of the math score and the EBRW score looks like it is closing. I did some digging and in 2018, the math section's standards were lowered! [Source](https://www.insidehighered.com/admissions/article/2018/07/12/surprisingly-low-scores-mathematics-sat-stun-and-anger-students) This might explain why the gap the is closing.\n",
    "\n",
    "3. While the EBRW interquartile range is slightly higher then math, math's maximum is consistently higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plot of EBRW and Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get average values for Math and EBRW\n",
    "col = ['2017','2018','2019']\n",
    "ebrw = [df['ebrw_avg_score_2017'].sum()/len(df),df['ebrw_avg_score_2018'].sum()/len(df),df['ebrw_avg_score_2019'].sum()/len(df)]\n",
    "math =[df['math_avg_score_2017'].sum()/len(df),df['math_avg_score_2018'].sum()/len(df),df['math_avg_score_2019'].sum()/len(df)]\n",
    "\n",
    "#Create new data frame\n",
    "df_avg_total = pd.DataFrame([ebrw,math],columns=col,index=['ebrw','math']).T\n",
    "\n",
    "#Plot\n",
    "df_avg_total.plot(kind='line',)\n",
    "\n",
    "#Labels and Title\n",
    "plt.title('EBRW and Math Average Results Across the USA')\n",
    "plt.ylabel('Average Scores')\n",
    "plt.xlabel('Years')\n",
    "\n",
    "#EBRW values on plot\n",
    "plt.text(0.055,569,'569')\n",
    "plt.text(0.983,564,'564')\n",
    "plt.text(1.83,561.7,'562')\n",
    "\n",
    "#Math values on plot\n",
    "plt.text(0.055,557.4,'557')\n",
    "plt.text(0.983,556.5,'556')\n",
    "plt.text(1.95,553.3,'552')\n",
    "\n",
    "plt.show()\n",
    "df_avg_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "Overall the average scores for EBRW and Math has declined over the three years from 2017-2019.\n",
    "However, EBRW has had the bigger drop of the two by 7 points as compared to the math score of 5 points.\n",
    "If anything, this looks like EBRW is a bigger concern. However once again keeping in mind that in 2018 the math section's difficulty was revised downwards this might be one of the factors holding up the overall results of the math scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "<details>\n",
    "<summary>Click to expand instructions</summary>\n",
    "Based on your exploration of the data, what are you key takeaways and recommendations? Make sure to answer your question of interest or address your problem statement here.\n",
    "    \n",
    "Don't forget to create your README!\n",
    "\n",
    "**To-Do:** *If you combine your problem statement, data dictionary, brief summary of your analysis, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.* Don't forget to cite your data sources!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few key takeaways we can take out of our analysis.\n",
    "\n",
    "1. There is a high negative correlaton between participation rates and scores. Could it be that states that mandate students to take the SAT result in unwilling participants that would put down the overall averages?\n",
    "2. From just these three years, it looks like the results are trending downwards for both math and EBRW\n",
    "3. There are more factors at play then meets the eye. For example the changing of the math section in 2018 and more recently the dropping of the EBRW essay section in 2021 [source](https://blog.collegeboard.org/January-2021-sat-subject-test-and-essay-faq)\n",
    "4. An interesting phenomenon that could be looked at would be why are the top scoring states in the mid-north of the country. And what are they doing differently that is helping the students in those states do well overall.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
