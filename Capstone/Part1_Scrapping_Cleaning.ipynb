{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5240e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# This is a .py file where I store all my functions to be used in the code\n",
    "import self_created_functions as scf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb453d",
   "metadata": {},
   "source": [
    "# Data Scrapping\n",
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b4ee3",
   "metadata": {},
   "source": [
    "In total I handpicked and scrapped 28 list of books from Goodreads using a scrapy spider created by [havanagraw1](https://github.com/havanagrawal/GoodreadsScraper).\n",
    "The spider grabs two different items. One on the books within the list and the other for the authors within the list. I will only be using the data on the books. More info can be found in the link above on how it works and the items scrapped.\n",
    "\n",
    "The lists I scrapped from goodreads are:\n",
    "\n",
    "|#|List Scrapped|\n",
    "|---|:---|\n",
    "|1|Best_Page_Turners_with_Redeeming_Social_Value|\n",
    "|2|Couldn_t_Put_The_Book_Down_|\n",
    "|3|Books_you_wish_more_people_knew_about_Part_II|\n",
    "|4|Best_Books_of_the_21st_Century|\n",
    "|5|Books_that_Blew_Me_Away_and_that_I_Still_Think_About_of_all_types_|\n",
    "|6|Best_Unknown_but_must_be_Known_books_|\n",
    "|7|1001_Books_You_Must_Read_Before_You_Die|\n",
    "|8|Books_That_Everyone_Should_Read_At_Least_Once|\n",
    "|9|Lesser_Known_Authors|\n",
    "|10|What_To_Read_Next|\n",
    "|11|The_Most_Influential_Books|\n",
    "|12|100_Books_to_Read_in_a_Lifetime_Readers_Picks|\n",
    "|13|Books_That_Should_Be_Made_Into_Movies|\n",
    "|14|Must_Read_Non_Fiction|\n",
    "|15|I_m_glad_someone_made_me_read_this_book|\n",
    "|16|Best_Books_Ever|\n",
    "|17|Books_With_a_Goodreads_Average_Rating_of_4_5_and_above_and_With_At_Least_100_Ratings|\n",
    "|18|Books_that_Changed_the_Way_You_View_Life|\n",
    "|19|100_Mysteries_and_Thrillers_to_Read_in_a_Lifetime_Readers_Picks|\n",
    "|20|Read_Them_Twice_At_Least|\n",
    "|21|Books_You_Wish_More_People_Knew_About|\n",
    "|22|Best_Young_Adult_Books|\n",
    "|23|Interesting_and_Readable_Nonfiction|\n",
    "|24|Best_Books_of_the_18th_Century|\n",
    "|25|Best_Books_of_the_Decade_2000s|\n",
    "|26|Best_for_Book_Clubs|\n",
    "|27|Best_Science_Fiction_Fantasy_Books|\n",
    "|28|Best_Books_of_the_Decade_1990s|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb397b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e89cd",
   "metadata": {},
   "source": [
    "As the data are saved into individual json files, I first concat them together into one dataframe and save it as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1052e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---START---\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_3187.Interesting_and_Readable_Nonfiction\n",
      "Rows: 2176, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_5.Best_Books_of_the_Decade_2000s\n",
      "Rows: 7074, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_17.Best_Books_of_the_Decade_1990s\n",
      "Rows: 3024, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_3.Best_Science_Fiction_Fantasy_Books\n",
      "Rows: 7675, Columns: 18\n",
      "Data not captured from url: 7668\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_43.Best_Young_Adult_Books\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_11618.Best_Page_Turners_with_Redeeming_Social_Value\n",
      "Rows: 3298, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_2994.Couldn_t_Put_The_Book_Down_\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_19253.Books_you_wish_more_people_knew_about_Part_II\n",
      "Rows: 3058, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_30\n",
      "Rows: 353, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_7.Best_Books_of_the_21st_Century\n",
      "Rows: 9406, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_2386.Books_that_Blew_Me_Away_and_that_I_Still_Think_About_of_all_types_\n",
      "Rows: 7749, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_25529.Best_Unknown_but_must_be_Known_books_\n",
      "Rows: 8603, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_952.1001_Books_You_Must_Read_Before_You_Die\n",
      "Rows: 1318, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_264.Books_That_Everyone_Should_Read_At_Least_Once\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1950.Lesser_Known_Authors\n",
      "Rows: 3894, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1938.What_To_Read_Next\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1083.The_Most_Influential_Books\n",
      "Rows: 1282, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_69635.100_Books_to_Read_in_a_Lifetime_Readers_Picks\n",
      "Rows: 8758, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1043.Books_That_Should_Be_Made_Into_Movies\n",
      "Rows: 9993, Columns: 19\n",
      "Data not captured from url: 9986\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_735.Must_Read_Non_Fiction\n",
      "Rows: 3392, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1190.I_m_glad_someone_made_me_read_this_book\n",
      "Rows: 8660, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1.Best_Books_Ever\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_10198.Books_With_a_Goodreads_Average_Rating_of_4_5_and_above_and_With_At_Least_100_Ratings\n",
      "Rows: 3113, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_1232.Books_that_Changed_the_Way_You_View_Life\n",
      "Rows: 5650, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_73283.100_Mysteries_and_Thrillers_to_Read_in_a_Lifetime_Readers_Picks\n",
      "Rows: 1750, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_474.Read_Them_Twice_At_Least\n",
      "Rows: 7407, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_8166.Books_You_Wish_More_People_Knew_About\n",
      "Rows: 10000, Columns: 19\n",
      "Data not captured from url: 0\n",
      "List Loaded:  raw_datasets/scrapped_json_files/book_19.Best_for_Book_Clubs\n",
      "Rows: 7773, Columns: 18\n",
      "Data not captured from url: 7765\n",
      "\n",
      "Concatenated Dataframe\n",
      "Rows: 175406, Columns: 19\n",
      "Total Null Values: 1168977\n",
      "Saved to: raw_datasets/book.csv\n",
      "---END---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab, Load, Concat and Save raw dataframe to csv from json files\n",
    "# To run code unzip the scrapped_json_file folder\n",
    "books = scf.create_df_from_json_files(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96e862",
   "metadata": {},
   "source": [
    "#### In total I scrapped 175,406 books however there are a lot of null values and possibly duplicates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4674c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>language</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>original_publish_year</th>\n",
       "      <th>genres</th>\n",
       "      <th>awards</th>\n",
       "      <th>characters</th>\n",
       "      <th>places</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>rating_histogram</th>\n",
       "      <th>series</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.goodreads.com/book/show/1845.Into_...</td>\n",
       "      <td>Into the Wild</td>\n",
       "      <td>Jon Krakauer</td>\n",
       "      <td>983231.0</td>\n",
       "      <td>24367.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>215.0</td>\n",
       "      <td>English</td>\n",
       "      <td>1997-01-20 00:00:00</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>[Environment, Travel, Survival, Biography Memo...</td>\n",
       "      <td>[Washington State Book Award (1997)]</td>\n",
       "      <td>[Christopher McCandless]</td>\n",
       "      <td>[Mexico, Virginia, The Slabs, Mojave Desert, C...</td>\n",
       "      <td>385486804.0</td>\n",
       "      <td>9.780385e+12</td>\n",
       "      <td>{'5': 363210, '4': 358039, '3': 186642, '2': 4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.goodreads.com/book/show/168484.Fem...</td>\n",
       "      <td>Feminism Is for Everybody: Passionate Politics</td>\n",
       "      <td>bell hooks</td>\n",
       "      <td>18885.0</td>\n",
       "      <td>1586.0</td>\n",
       "      <td>4.14</td>\n",
       "      <td>123.0</td>\n",
       "      <td>English</td>\n",
       "      <td>2000-10-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Social Movements, Politics, Sociology, Race, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>896086283.0</td>\n",
       "      <td>9.780896e+12</td>\n",
       "      <td>{'5': 8328, '4': 6498, '3': 2843, '2': 792, '1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.goodreads.com/book/show/55403.Blac...</td>\n",
       "      <td>Black Hawk Down: A Story of Modern War</td>\n",
       "      <td>Mark Bowden</td>\n",
       "      <td>59451.0</td>\n",
       "      <td>1727.0</td>\n",
       "      <td>4.28</td>\n",
       "      <td>386.0</td>\n",
       "      <td>English</td>\n",
       "      <td>1999-02-10 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[War, Africa, North American Hi..., Cultural, ...</td>\n",
       "      <td>[National Book Award Finalist for Nonfiction (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Mogadishu]</td>\n",
       "      <td>871137380.0</td>\n",
       "      <td>9.780871e+12</td>\n",
       "      <td>{'5': 28462, '4': 21639, '3': 7502, '2': 1252,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.goodreads.com/book/show/1845.Into_...   \n",
       "1  https://www.goodreads.com/book/show/168484.Fem...   \n",
       "2  https://www.goodreads.com/book/show/55403.Blac...   \n",
       "\n",
       "                                            title        author  num_ratings  \\\n",
       "0                                   Into the Wild  Jon Krakauer     983231.0   \n",
       "1  Feminism Is for Everybody: Passionate Politics    bell hooks      18885.0   \n",
       "2          Black Hawk Down: A Story of Modern War   Mark Bowden      59451.0   \n",
       "\n",
       "   num_reviews  avg_rating  num_pages language         publish_date  \\\n",
       "0      24367.0        4.00      215.0  English  1997-01-20 00:00:00   \n",
       "1       1586.0        4.14      123.0  English  2000-10-01 00:00:00   \n",
       "2       1727.0        4.28      386.0  English  1999-02-10 00:00:00   \n",
       "\n",
       "   original_publish_year                                             genres  \\\n",
       "0                 1996.0  [Environment, Travel, Survival, Biography Memo...   \n",
       "1                    NaN  [Social Movements, Politics, Sociology, Race, ...   \n",
       "2                    NaN  [War, Africa, North American Hi..., Cultural, ...   \n",
       "\n",
       "                                              awards  \\\n",
       "0               [Washington State Book Award (1997)]   \n",
       "1                                                NaN   \n",
       "2  [National Book Award Finalist for Nonfiction (...   \n",
       "\n",
       "                 characters  \\\n",
       "0  [Christopher McCandless]   \n",
       "1                       NaN   \n",
       "2                       NaN   \n",
       "\n",
       "                                              places         isbn  \\\n",
       "0  [Mexico, Virginia, The Slabs, Mojave Desert, C...  385486804.0   \n",
       "1                                                NaN  896086283.0   \n",
       "2                                        [Mogadishu]  871137380.0   \n",
       "\n",
       "         isbn13                                   rating_histogram series asin  \n",
       "0  9.780385e+12  {'5': 363210, '4': 358039, '3': 186642, '2': 4...    NaN  NaN  \n",
       "1  9.780896e+12  {'5': 8328, '4': 6498, '3': 2843, '2': 792, '1...    NaN  NaN  \n",
       "2  9.780871e+12  {'5': 28462, '4': 21639, '3': 7502, '2': 1252,...    NaN  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73eda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features that holds no relevance to what we want to do\n",
    "unwanted_book_cols = ['asin','isbn','isbn13','characters','places','publish_date','series','rating_histogram','awards']\n",
    "books_clean = books.drop(unwanted_book_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965c405a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                       object\n",
       "title                     object\n",
       "author                    object\n",
       "num_ratings              float64\n",
       "num_reviews              float64\n",
       "avg_rating               float64\n",
       "num_pages                float64\n",
       "language                  object\n",
       "original_publish_year    float64\n",
       "genres                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ea0c4",
   "metadata": {},
   "source": [
    "### Duplicate Items\n",
    "The different lists might contain the same books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8eb7588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108468"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Num of duplicates\n",
    "books_clean['title'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dc87cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "books_clean.drop_duplicates(subset=['title'],keep='first',inplace=True)\n",
    "books_clean['title'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3dcada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping duplicates we are left with: 66938, books!\n"
     ]
    }
   ],
   "source": [
    "print(f'After dropping duplicates we are left with: {books_clean.shape[0]}, books!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedff94",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd544bc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url                          0\n",
      "title                        1\n",
      "author                       1\n",
      "num_ratings                  1\n",
      "num_reviews                  1\n",
      "avg_rating                   1\n",
      "num_pages                 4490\n",
      "language                  6380\n",
      "original_publish_year    31383\n",
      "genres                    9865\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>language</th>\n",
       "      <th>original_publish_year</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12281</th>\n",
       "      <td>https://www.goodreads.com/book/show/25499718-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url title author  \\\n",
       "12281  https://www.goodreads.com/book/show/25499718-c...   NaN    NaN   \n",
       "\n",
       "       num_ratings  num_reviews  avg_rating  num_pages language  \\\n",
       "12281          NaN          NaN         NaN        NaN      NaN   \n",
       "\n",
       "       original_publish_year genres  \n",
       "12281                    NaN    NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There's a row that except for the url are null\n",
    "print(books_clean.isnull().sum())\n",
    "books_clean[books_clean.title.isnull()].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66ff9c",
   "metadata": {},
   "source": [
    "Looking at the above null rows and matching it with the data not captured from url when loading the json file [here](#Data-Cleaning), it looks like the spider is unable to scrape the lists: \"Books_That_Should_Be_Made_Into_Movies\", \"Best_Science_Fiction_Fantasy_Books\", and \"Best_for_Book_Clubs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05367681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows with title as blank\n",
    "books_clean = books_clean[books_clean.title.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1d48c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                          0\n",
       "title                        0\n",
       "author                       0\n",
       "num_ratings                  0\n",
       "num_reviews                  0\n",
       "avg_rating                   0\n",
       "num_pages                 4489\n",
       "language                  6379\n",
       "original_publish_year    31382\n",
       "genres                    9864\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d386fe",
   "metadata": {},
   "source": [
    "To fill in the remaining null values I shall try scrapping from wikipedia's info box using [wptools](https://pypi.org/project/wptools/)\n",
    "![](./images/infobox.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfef059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: This takes 1-2 hours to run!\n",
      "Input Y to run, or any other letters to skip: n\n"
     ]
    }
   ],
   "source": [
    "# Scrape wikipedia info-box for null values, clean the data and save to csv\n",
    "# Run code, type \"Y\" to run function or any other letters to skip\n",
    "scf.wptools_scrape_save_tocsv(df = books_clean,save_to =\"./raw_datasets/missing_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16267660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (46, 6)\n",
      "Null Values:\n",
      "name         0\n",
      "author       0\n",
      "language     5\n",
      "pages        1\n",
      "genre        5\n",
      "pub_date    25\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>language</th>\n",
       "      <th>pages</th>\n",
       "      <th>genre</th>\n",
       "      <th>pub_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The October Horse</td>\n",
       "      <td>[[Colleen McCullough]]</td>\n",
       "      <td>English</td>\n",
       "      <td>608.0</td>\n",
       "      <td>Historical, novel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany: Memories of a Nation</td>\n",
       "      <td>[[Neil MacGregor]]</td>\n",
       "      <td>English</td>\n",
       "      <td>598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darwin's Radio</td>\n",
       "      <td>[[Greg Bear]]</td>\n",
       "      <td>English</td>\n",
       "      <td>448.0</td>\n",
       "      <td>Science, fiction, novel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name                  author language  pages  \\\n",
       "0              The October Horse  [[Colleen McCullough]]  English  608.0   \n",
       "1  Germany: Memories of a Nation      [[Neil MacGregor]]  English  598.0   \n",
       "2                 Darwin's Radio           [[Greg Bear]]  English  448.0   \n",
       "\n",
       "                     genre pub_date  \n",
       "0        Historical, novel      NaN  \n",
       "1                      NaN     2014  \n",
       "2  Science, fiction, novel      NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the scrapped wikipedia - csv file scrapped above\n",
    "missing_info = pd.read_csv(\"./raw_datasets/missing_info.csv\")\n",
    "#Keep only features wanted\n",
    "missing_info = missing_info[['name','author','language','pages','genre','pub_date']]\n",
    "print('Shape:',missing_info.shape)\n",
    "print('Null Values:')\n",
    "print(missing_info.isnull().sum())\n",
    "missing_info.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d43d6",
   "metadata": {},
   "source": [
    "It looks like wikipedia's info-box isn't as complete as I wanted and the maximum possible books with missing info I can plug in is 46. Nevertheless I shall plug in as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3bf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of the missing_info columns to match the main df books\n",
    "missing_info.rename(columns={'name':'title',\n",
    "                             'pages':'num_pages',\n",
    "                             'genre':'genres',\n",
    "                             'pub_date':'original_publish_year'\n",
    "                            },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c03fdbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null values filled in 52\n"
     ]
    }
   ],
   "source": [
    "# Comparing two dataframes, replace the reference dataframe\n",
    "# with the main dataframes index so that I can use combine_first\n",
    "# to fill in the null value for the main dataframe\n",
    "scf.index_conversion(books_clean,missing_info,'title','num_pages')\n",
    "df = books_clean.combine_first(missing_info)\n",
    "\n",
    "print('Total Null values filled in',books_clean.isnull().sum().sum()-df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778e43c",
   "metadata": {},
   "source": [
    "Next we shall try using the library [langid](https://github.com/saffsd/langid.py) to detect the language of the titles of books and fill in the null value for the column language.\n",
    "\n",
    "langid returns the iso 69-1 codes of the different languages. to get the language itself, we need to reference the table from wikipedia\n",
    "\n",
    "steps taken\n",
    "1. Import langid and use it to get a list of languages referencing the title of the books\n",
    "2. Scrape wikipedia ISO codes page and extract the table of codes\n",
    "3. Map the codes to the actual language\n",
    "4. Fill in the null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb770e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "languages detected: {'jv', 'ar', 'fi', 'mg', 'tl', 'hi', 'cy', 'ms', 'be', 'eu', 'sl', 'da', 'ko', 'pt', 'it', 'sq', 'cs', 'no', 'eo', 'hu', 'mr', 'et', 'ca', 'la', 'ro', 'xh', 'ps', 'bg', 'az', 'vi', 'de', 'sk', 'uk', 'fr', 'sv', 'es', 'el', 'mt', 'ga', 'id', 'hr', 'tr', 'fa', 'nl', 'pl', 'en', 'lt'}\n"
     ]
    }
   ],
   "source": [
    "# Get a list of titles with language as null\n",
    "lang_null = list(df['title'][df.language.isnull()])\n",
    "\n",
    "# set function to check title's language\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Append to a list\n",
    "lang=[identifier.classify(title)[0] for title in lang_null]\n",
    "print('languages detected:',set(lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "770db7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO language name</th>\n",
       "      <th>639-1</th>\n",
       "      <th>639-2/T</th>\n",
       "      <th>639-2/B</th>\n",
       "      <th>639-3</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abkhazian</td>\n",
       "      <td>ab</td>\n",
       "      <td>abk</td>\n",
       "      <td>abk</td>\n",
       "      <td>abk</td>\n",
       "      <td>also known as Abkhaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afar</td>\n",
       "      <td>aa</td>\n",
       "      <td>aar</td>\n",
       "      <td>aar</td>\n",
       "      <td>aar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "      <td>afr</td>\n",
       "      <td>afr</td>\n",
       "      <td>afr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ISO language name 639-1 639-2/T 639-2/B 639-3                 Notes\n",
       "0         Abkhazian    ab     abk     abk   abk  also known as Abkhaz\n",
       "1              Afar    aa     aar     aar   aar                   NaN\n",
       "2         Afrikaans    af     afr     afr   afr                   NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the language table for the 639-1 codes from wikipedia\n",
    "iso_url = \"https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\"\n",
    "response = requests.get(iso_url)\n",
    "print('Response',response.status_code)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "table = soup.find('table',{'class':\"wikitable\"})\n",
    "iso = pd.read_html(str(table))\n",
    "iso_df = pd.DataFrame(iso[0])\n",
    "iso_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a90d547",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Map the 639-1 code to the ISO language name\n",
    "lang = [iso_df['ISO language name'][iso_df[iso_df['639-1']==x].index[0]] for x in lang]\n",
    "\n",
    "# Fill in the null values..\n",
    "lang_null_index = list(df['title'][df.language.isnull()].index)\n",
    "for i,x in zip(lang_null_index,lang):\n",
    "    df.loc[i,'language'] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d282d",
   "metadata": {},
   "source": [
    "For the reminding missing values, due to time constraint I shall not try to scrape any further. Instead I shall drop the column original_publish_year, as more then half of the book's publish year is missing. Then I shall drop the remaining books that still have a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ab7f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're left with 54478 books in our dataframe\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['original_publish_year'],axis=1,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(f\"We're left with {len(df)} books in our dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742c508",
   "metadata": {},
   "source": [
    "### Cleaning the column genres\n",
    "Genres are strings within a list. Some of the genres are combined together eg. SurvivalFiction while others are split apart Non fiction. some are plural etc.\n",
    "\n",
    "Example: ['Sports', 'Journalism', 'Soccer', 'History', 'Sociology', 'Football', 'British Literature', 'Writing', 'European Literature', 'Crime', 'Autobiography', 'TrueCrime', 'Memoir', 'Nonfiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa41017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Genres and subGenres:  891\n"
     ]
    }
   ],
   "source": [
    "# Remove all non alphabetical characters\n",
    "df['genres']=df['genres'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', str(x)))\n",
    "\n",
    "# Remove front and back spacing and lowercase all\n",
    "df['genres']=df['genres'].apply(lambda x: x.strip().lower())\n",
    "\n",
    "# Convert back to a list\n",
    "df['genres'] = df['genres'].apply(lambda x: x.split())\n",
    "\n",
    "#Lemmatize\n",
    "df = scf.lemma_column(df,'genres')\n",
    "\n",
    "# Convert to str\n",
    "df['genres'] = df['genres'].apply(lambda x: ' '.join(ele for ele in x))\n",
    "\n",
    "#remove stopwords\n",
    "df = scf.remove_stopwords(df,'genres')\n",
    "\n",
    "# Drop the \"s\" at the end of every word\n",
    "df['genres'] = df['genres'].apply(lambda x: ' '.join(ele[:-1] if ele[-1] == 's' else ele for ele in x.split()))\n",
    "\n",
    "print('Total Number of Genres and subGenres: ',len(set(scf.flatten([x.split() for x in [x for x in df['genres']]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d437284",
   "metadata": {},
   "source": [
    "Each book's list of genres can be as short as 1 and as long as 202. Without manually going into each book and editing the genre I would not be able to get a standardise set of genres to use as a feature for my recommender system (example, fiction vs nonfiction). As such I shall leave it as it is for now... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42943611",
   "metadata": {},
   "source": [
    "### Cleaning the column author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e67899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c033dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "#cannot use regex here as there're authors in other languages\n",
    "to_replace = [\".\",\",\",\"/\",\"'\"]\n",
    "for ele in to_replace:\n",
    "    df['author'] = df['author'].apply(lambda x: x.replace(ele,\" \"))\n",
    "\n",
    "#Remove extra whitespace\n",
    "df['author']=df['author'].apply(lambda x: (' '.join(x.split())).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c70bc35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[title     Mr. Darcy Goes Overboard: A Tale of Tide & Pre...\n",
       " author                                                    j\n",
       " url       https://www.goodreads.com/book/show/10425347-m...\n",
       " Name: 14259, dtype: object,\n",
       " title                                              Shifters\n",
       " author                                                   dp\n",
       " url       https://www.goodreads.com/book/show/18292964-s...\n",
       " Name: 15087, dtype: object]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for empty strings or short named authors\n",
    "check = {i for i in range(len(df)-1) if len(df['author'][i]) <3}\n",
    "\n",
    "[df[['title','author','url']].iloc[i] for i in check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f7006",
   "metadata": {},
   "source": [
    "After cleaning, there are two who's author is just one or two letters. Going into the URL we can get the author's name and then rename the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d241bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[14259,'author'] = 'belinda roberts'\n",
    "df.loc[15087,'author'] = 'douglas pershing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330478bc",
   "metadata": {},
   "source": [
    "### Cleaning the column language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "096ec96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English', 'Dutch', 'French', 'Indonesian', 'German', 'Portuguese',\n",
       "       'Spanish, Castilian', 'Finnish', 'Norwegian', 'Spanish', 'Polish',\n",
       "       'Japanese', 'Italian', 'Russian', 'Swedish', 'Ukrainian',\n",
       "       'Bulgarian', 'Tamil', 'Danish', 'Afrikaans',\n",
       "       'Norwegian Nynorsk; Nynorsk, Norwegian', 'Filipino; Pilipino',\n",
       "       'Greek, Modern (1453–)', 'Greek, Modern (1453-)', 'Hungarian',\n",
       "       'Estonian', 'Bokmål, Norwegian; Norwegian Bokmål', 'Scots',\n",
       "       'Dutch, Flemish', 'Romanian', 'Persian', 'Czech', 'Arabic',\n",
       "       'Hindi', 'Catalan; Valencian', 'Tagalog', 'Malay', 'Latvian',\n",
       "       'Vietnamese', 'Dutch, Middle (ca.1050-1350)', 'Turkish', 'Korean',\n",
       "       'Slovenian', 'Multiple languages', 'Chinese', 'Xhosa', 'Basque',\n",
       "       'Lithuanian', 'Malayalam', 'Hebrew', 'Esperanto', 'Georgian',\n",
       "       'Urdu', 'Albanian', 'Marathi', 'Maltese', 'Serbian', 'Croatian',\n",
       "       'Greek, Ancient (to 1453)', 'Catalan, Valencian', 'Galician',\n",
       "       'English, Middle (1100-1500)', 'Kannada', 'English,', 'Telugu',\n",
       "       'Latin', 'Malagasy', 'Amharic', 'Irish', 'Bengali',\n",
       "       'Romanian, Moldavian, Moldovan', 'Icelandic', 'Thai', 'Bosnian'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f68398",
   "metadata": {},
   "source": [
    "There are a few values that are tagged with extra info like English, Middle (1100-1500) or more then one version of the language attached: 'Romanian, Moldavian, Moldovan'. Having this granularity only for certain rows would skrew the data. So we'll keep only the first language given in the cell and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26b2d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non alphabetical characters\n",
    "df['language']=df['language'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', str(x)))\n",
    "\n",
    "# keep only the first text\n",
    "# Bokm is a unique case as there's Bokm Norwegian and Norwegian\n",
    "df['language']=df['language'].apply(lambda x: 'Norwegian' if x[:4]=='Bokm' else x[:scf.blank_index(x)].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cafc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save cleaned dataset for EDA and ML\n",
    "df.to_csv('./cleaned_datasets/books_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2965ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
